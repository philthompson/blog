
<!-- Copyright 2024 Phil Thompson. All Rights Reserved.  As noted in the License section of this repository's readme.md file, this file and its corresponding public HTML file, and all other articles, article files, and images, are distributed under traditional copyright.  The repository source code and other files are distributed under the MIT license. -->

[//]: # (gen-title: Counting Polycubes of Size 21)

[//]: # (gen-title-url: Counting-Polycubes-of-Size-21)

[//]: # (gen-keywords: polycubes, geometry, math, python, rust, C#, OEIS, AWS, EC2, Microsoft, Copilot, Bing, GPT4, porting, threads, multithreading)

[//]: # (gen-description: Discussion of my efforts to write and run code to count polycubes.)

[//]: # (gen-meta-end)

<a href="${THIS_ARTICLE}"><img style="float: left" class="width-resp-50-100" src="https://philthompson.me/s/img/2024/20240119-polycubes.jpg"/></a> There are 1,636,229,771,639,924 unique 3D polycubes that can be constructed with 21 cubes.

I calculated this number over a period of 10 days in the <a target="_blank" href="https://aws.amazon.com/ec2/">Amazon Elastic Compute Cloud</a> using <a target="_blank" href="https://oeis.org/wiki/User:Stanley_Dodds">Stanley Dodds's</a> amazing algorithm, which I ported to rust.  In this post I'll give some background on my efforts to compute this value.

The canonical source of many integer sequences, including many polycube and polyomino sequences, lives at the OEIS, where this (polycubes containing <math style="font-size:1.2rem"><mi>n</mi></math> cubes) is <a target="_blank" href="https://oeis.org/A000162">sequence A000162</a>.  I plan to extend that sequence with this value.

[more](more://)

<div class="wrap-wider-child">
	<img class="width-100 center-block" src="https://philthompson.me/s/img/2024/20230119-n12-animation.gif"/>
</div>

<small>Above, we have some screenshots from <a target="_blank" href="https://github.com/Wagyx">Wagyx's</a> <a target="_blank" href="https://asliceofcuriosity.fr/assets/polycube-viewer/polycube-viewer.html">viewer app</a> (see discussion of this app <a target="_blank" href="https://github.com/mikepound/opencubes/issues/41">here</a>).</small>

A mid-2023 <a target="_blank" href="https://www.youtube.com/watch?v=g9n0a0644B4">Computerphile video on YouTube</a> was about writing code to count polycubes.  The idea is to count the unique 3D shapes that can be formed by joining together some number, <math style="font-size:1.2rem"><mi>n</mi></math>, cubes on their faces.  A "unique shape" is one that cannot be rotated to match another shape.  Some pairs of shapes mirror each other, where one cannot be rotated in any direction(s) to match the other.  These pairs count as two unique shapes.  One example is this pair of <math style="font-size:1.2rem"><mi>n</mi><mo>=</mo><mn>4</mn></math> shapes shown on <a target="_blank" href="https://en.wikipedia.org/wiki/Polycube">Wikipedia's Polycube article</a>:

<img class="width-100 center-block" src="https://philthompson.me/s/img/2024/20240119-polycube-mirror-shapes.png"/> 

<small>(This above image is based on an image appearing on that Wikipedia page, and is thus released under the CC BY-SA 4.0 license.)</small>

There is no straightforward way to simply calculate the number of polycubes of size <math style="font-size:1.2rem"><mi>n</mi></math> &mdash; as far as we currently know, they have to be individually counted.  This problem is deceptively difficult.  <a target="_blank" href="https://oeis.org/A000162">According to the OEIS, as of 2007</a> we (as a species) had only been able to enumerate the polycubes up to <math style="font-size:1.2rem"><mi>n</mi><mo>=</mo><mn>16</mn></math>!  While 16 doesn't seem like a lot of cubes, apparently 59,795,121,480 unique 16-cube shapes exist.

## Algorithms

The Computerphile video does a nice job of laying out the problem.  There are 24 possible rotations for each shape in 3 dimensions, so a straightforward algorithm computes all possible shapes, and keeps a hash table or similar to prevent counting duplicates.  Then for each shape, apply all 24 rotations and take the hash of each rotated shape.  If any one of those hashes appears in the hash table, disregard that shape as a duplicate.  Otherwise, add one of those hashes to the hash table.  As <math style="font-size:1.2rem"><mi>n</mi></math> grows to 16, 17, 18, etc., that table would quickly exhaust any computer's memory.

The best solutions would therefore not involve a table at all.  I tried to think of a way to hash polycubes independent of rotation, and quickly ran into the limits of my own abilities.

<a target="_blank" href="https://github.com/presseyt">presseyt on GitHub</a> produced a nice algorithm for table-less counting, along with some <a target="_blank" href="https://github.com/presseyt/opencubes/tree/main/javascript">nicely-performing JavaScript code</a>.  I was able to port the JavaScript to python, and running with `pypy`, it was much faster.  presseyt's algorithm is perfectly parallelizable: it can be split into individual smaller subtasks (even millions or billions of them) that can be completed in complete isolation.  This sort of algorithm could theoretically be run over a huge distributed network.

I then spent a few weeks porting my python to rust, and optimizing that code.  I ended up with a decent result (<a target="_blank" href="https://github.com/philthompson/polycubes">here's my GitHub repo</a>), but computing for <math style="font-size:1.2rem"><mi>n</mi><mo>=</mo><mn>17</mn></math> would take 30 days or more on my M1 Mac mini.  My realistic goal at this point was to calculate for <math style="font-size:1.2rem"><mi>n</mi><mo>=</mo><mn>17</mn></math> and <math style="font-size:1.2rem"><mi>n</mi><mo>=</mo><mn>18</mn></math> or so, which at the time were not listed on <a target="_blank" href="https://oeis.org/A000162">https://oeis.org/A000162</a>.

*Note: I was unaware that by this time some folks, including <a target="_blank" href="https://github.com/pbj4">pbj4 on GitHub</a>, had already computed up to <math style="font-size:1.2rem"><mi>n</mi><mo>=</mo><mn>19</mn></math> and had been <a target="_blank" href="https://github.com/mikepound/opencubes/issues/42">posting those results to the GitHub repo associated with the Computerphile video</a>.*

## A New Algorithm

Then I noticed that the OEIS entry had been updated!  After 16 years the sequence had been extended while I was working on this problem!  Counts for <math style="font-size:1.2rem"><mi>n</mi><mo>=</mo><mn>17</mn></math> and <math style="font-size:1.2rem"><mi>n</mi><mo>=</mo><mn>18</mn></math> were added, and also <math style="font-size:1.2rem"><mi>n</mi><mo>=</mo><mn>19</mn></math> and even <math style="font-size:1.2rem"><mi>n</mi><mo>=</mo><mn>20</mn></math>!

I don't know how coincidental it was for the Computerphile video to come out in mid-2023, then, later that year, the OEIS entry was extended for the first time since 2007.  I would imagine it's not very coincidental.

The OEIS entry was updated by <a target="_blank" href="https://oeis.org/wiki/User:Stanley_Dodds">Stanley Dodds</a>.  Their update included C# source code, presumably the code used to calculate the, unthinkable just days prior, count for <math style="font-size:1.2rem"><mi>n</mi><mo>=</mo><mn>20</mn></math>.  Luckily, I already had `mono` installed on my M1 mac, and was able to compile and run the C#.  As a test run, it output for <math style="font-size:1.2rem"><mi>n</mi><mo>=</mo><mn>11</mn></math> *in less than one second*, where my previous best, with a rust program, took 45 seconds.  Wow!  45 seconds down to just *nothing* for <math style="font-size:1.2rem"><mi>n</mi><mo>=</mo><mn>11</mn></math>!?  Then I tried <math style="font-size:1.2rem"><mi>n</mi><mo>=</mo><mn>17</mn></math>.  After about 25 minutes (with 8 threads on my M1 Mac mini), it returned the proper count: beyond 457 billion.  This was truly mind-blowing performance.  My previous best solution would have taken more than 30 days!  And this was C# code!

While I fully understand presseyt's algorithm, I haven't yet taken the time to do the same for the overall methodology of Dodds's. The first portion of Dodds's incredible algorithm counts "nontrivial symmetries," then, separately, counts "trivial symmetries." As far as I can tell, the second portion is some sort of traversal using stacks and a table to somehow count all rotations of all polycubes of <math style="font-size:1.2rem"><mi>n</mi></math> cubes, without repeating any.  For the final step, it simply sums the counts from the first and second portions, then divides that sum by 24 to end up at the count of unique shapes.

## A New Goal

My goal was instantly changed: calculating for <math style="font-size:1.2rem"><mi>n</mi><mo>=</mo><mn>21</mn></math> and <math style="font-size:1.2rem"><mi>n</mi><mo>=</mo><mn>22</mn></math> now seemed possible.  To calculate those, I figured it would make sense to port Stanley Dodds's code to rust and then to run it on a beefy machine in the cloud.

To save some time with the port, and also to try out the latest LLM <s>toy</s> tool, I tried out Microsoft's new Copilot app on the Mac.  By carefully copy/pasting in small portions of the C# code, and asking to port each one to rust, I quickly had a rough outline of the rust port.  Using Copilot was complicated by the limit on the input size, but it produced mostly usable rust code.  Overall though it probably didn't save me a ton of time.  I then spent several days correcting obvious errors in gpt4's code, and finding and fixing a few harder-to-find problems.  I was finally able to get correct output with the rust version of Dodds's algorithm, and it seemed to be several times faster than the C# version.

This is where I made a few small mistakes that set me back a few days.  At this point, I was excited to start calculating for <math style="font-size:1.2rem"><mi>n</mi><mo>=</mo><mn>21</mn></math>, and I hurried to get my rust code ready to run on an AWS EC2 instance.  I added multi-threaded calculation of "nontrivial symmetries," the faster first portion of the calculation, and the ability to hard code those results so they don't have to be re-calculated every time the program is run.  I added some code to output running time and give an estimated time remaining.

After confirming how many worker thread tasks the counting could be split among (it was 63 tasks for <math style="font-size:1.2rem"><mi>n</mi><mo>=</mo><mn>21</mn></math>) I needed to see if it could be completed in a realistic amount of time running on an AWC EC2 instance.  I then ran my rust code on my home M1 Mac mini to see how long each thread task would take.  I was really excited to see that my home machine finished one worker task in less than 24 hours!  With that running time confirmed, I thought I could fire up an AWS instance with many CPUs and get a result relatively quickly.

I looked at 64-vCPU AWS EC2 pricing.  I saw the `c7g` and `c6g` instances were the cheapest, but they were grayed out in the instance type selection menu.  Since I figured the running time wouldn't be very long, and it doesn't require a lot of memory, I shrugged off the `c7g` / `c6g` instance types and launched the cheapest compute-optimized 64-vCPU instance type I could find: a `c7a.16xlarge` instance.

While running my initial rust code in EC2 for a couple days, I made a few realizations.  I stopped that instance, sacrificing my partially-computed results in the process.

## Initial AWS EC2 Mistakes

The first mistake I made there was not printing the actual count found for each worker thread task.  Since each worker thread task finds its own completely independent count (enumeration of a subset of the shapes of polycubes of <math style="font-size:1.2rem"><mi>n</mi></math> cubes), if I had simply printed those tasks' counts to the screen, I could have re-used them to resume computation.

This leads me to my next mistake: not realizing that worker tasks do not all take the same time to run!  In fact, the slowest worker tasks take about **five times** as long to run as the fastest ones:

<img class="width-100 center-block" src="https://philthompson.me/s/img/2024/20240119-polycube-thread-task-times.png"/> 

This means the overall running time is about 5 times as long as I thought it would be (since I was using a separate vCPU and thread for each task).  This also means that if starting 63 threads at once, one for each worker task, the fast ones will finish and then sit idle (for days!) while the long-running jobs keep running.  I saw hints of this while testing the code, but I didn't stop to investigate this enough.

The next mistake I made here was a real forehead-slapper.  I was developing my rust code using `cargo` and testing with `cargo run  --release` to compile and run with performance optimizations.  On the EC2 instance, I didn't want to bother setting up a cargo project.  I kept my code constrained to a single source file, without using any external crates, so I just compiled with `rustc polycubes.rs` &mdash; which doesn't compile with optimizations!  This means the code I was running was running at half speed, and actually probably slower than that, compared to the speed I was seeing with `cargo run  --release` on my machine.  If I had just instead used `rustc -C opt-level=3 polycubes.rs` the code would have run at the expected speed, more than twice as fast. 

The last mistake I'll list here has to do with AWS EC2 instance types.  The `g` AWS EC2 instances <a target="_blank" href="https://aws.amazon.com/ec2/instance-types/c7g/">are ARM-based</a>, and presumably cost Amazon a lot less power to run.  They're the cheapest compute-optimized instance type, which I did initially see, but I didn't think my running times would be long enough to matter so I just ignored the fact they those "g" instances didn't appear to be available.  I didn't realize the EC2 web console UI limited the instance types by the architecture of the selected AMI.  To run the cheaper and faster "g" instances, I needed to browse the full list of AMIs, then filter for ARM AMIs &mdash; for example when using the Amazon Linux at the top of the ARM AMIs list, that itself needed another option to be toggled to use ARM.  In my defense, I clicked the big "Amazon Linux" button, which does have an ARM variant...

## AWS Improvements

The few longest-running worker tasks are at the end of the list of tasks to run.  Most of the tasks, including just about the first half of all tasks, are the fastest.  I realized that by running the tasks in reverse order, we can start the few long-running tasks first.  While those long-running tasks continue to run (again, some of which take about five times as long to run as a the short tasks), an individual vCPU has plenty of time to run several faster tasks.  This means that by running the tasks in reverse order, **and** with a smaller number of vCPUs, we'll have the same overall run time, less idle vCPU time, and less overall AWS cost.  Because of that 5x runtime factor, I figured it would be safe to run with 1/4 of the vCPUs I was previously running &mdash; 16 vs the original 64.  This means the AWS EC2 pricing would be 1/4 the cost of my initial attempt.  (See the [Running times and costs for tasks and vCPUs](#running-time-cost-vs-threads) section for more information on this.)

I also added printing of timestamps and counts when each individual task is completed.  After hard coding any previously-calculated task results, by first doing a lookup before kicking off a task, we can skip any task whose count has already been calculated.  This will save me in case my instances are killed.

Next, I experimented with the performance of the "g" instance types.  Both `c6g` and `c7g` are much cheaper than the `c7a` instance I was initially using &mdash; and I found `c7g` to be ~10% more expensive but well more than 10% faster than `c6g`.

Lastly, I compiled my single rust source file with `rustc -C opt-level=3 polycubes.rs`.  This alone sped things up by more than 2x.

## <a name="running-time-cost-vs-threads"></a>Running Time+Cost vs. Threads

For EC2 instances in the AWS cloud, hourly on-demand pricing generally scales linearly with the number of vCPUs.  For example, a 64-vCPU instance generally costs twice as much per hour as a 32-vCPU instance.  With this in mind, we can use the number of thread tasks that need to be run, and their individual running times, to find an instance size that balances overall running time and cost.

Of course, we don't know the exact running time before we actually run the code.  But by looking at the running times for thread tasks for smaller values of <math style="font-size:1.2rem"><mi>n</mi></math>, I felt like I could see how the running times increased relative to an increase in <math style="font-size:1.2rem"><mi>n</mi></math>.  *After* I launched my second attempt at <math style="font-size:1.2rem"><mi>n</mi><mo>=</mo><mn>21</mn></math> in AWS, on a 16-vCPU instance, I generated the below data to double-check my selected instance size.

Here, I've charted the running times of each thread for <math style="font-size:1.2rem"><mi>n</mi><mo>=</mo><mn>14</mn></math>, <math style="font-size:1.2rem"><mi>n</mi><mo>=</mo><mn>16</mn></math>, and <math style="font-size:1.2rem"><mi>n</mi><mo>=</mo><mn>18</mn></math>.  Both the thread running times and the thread numbers have been scaled as a proportion of the maximum, so everything is aligned:

<img class="width-100 center-block" src="https://philthompson.me/s/img/2024/20240119-compare-thread-times.png"/>

Every increment of <math style="font-size:1.2rem"><mi>n</mi></math> adds another 8 thread tasks, which for this chart means the <math style="font-size:1.2rem"><mi>n</mi><mo>=</mo><mn>18</mn></math> line shows the individual running times of 51 tasks.  Interestingly, the 10th task always seems to be the longest running, and the peak of each of these lines is at the 10th task.

We can see that as <math style="font-size:1.2rem"><mi>n</mi></math> is increased, the *duration* of the longest-running threads is **not** increased relative to the fast threads.  Also, we can see the "hump" gets more narrow as <math style="font-size:1.2rem"><mi>n</mi></math> increases &mdash; this shows that the proportion of long-running threads actually **decreases** as <math style="font-size:1.2rem"><mi>n</mi></math> is increased.  As <math style="font-size:1.2rem"><mi>n</mi></math> increases, having the same long/short task duration ratio and with proportionally fewer long-running threads, we don't need to dedicate more vCPUs for long-running threads.  Instead, we could use more vCPUs to reduce the overall running time without incurring too much wasted cost of idle vCPU time.  On the other hand, since AWS generally only offers 8-, 16-, 32- and 64-vCPU instances that are relevant here (and nothing in between those vCPU quantities) it doesn't seem like it makes that much of a difference.

### Charting for <math style="font-size:1.4rem"><mi>n</mi><mo>=</mo><mn>18</mn></math>

As we change the number of threads a hypothetical cloud machine can run, we affect both the running time and the total cost of running the calculation.  For the below chart, I've plotted both running time and cost for 1-60 threads for <math style="font-size:1.2rem"><mi>n</mi><mo>=</mo><mn>18</mn></math>.  I've added a dark gray line for available AWS EC2 instance sizes at 8, 16, and 32 threads.  The blue and green running time lines are plotted on a log scale, and the exact units are omitted here since this is a hypothetical machine.  The magenta and purple are plotted on a linear scale, and again I'm omitting the units of the hypothetical cost (calculated with <math style="font-size:1.2rem"><mtext>threads</mtext><mo>×</mo><mtext>time</mtext></math>).  When "reversed," we start with the long-running thread tasks, and "nonrev" we start with the fast thread tasks (see the chart above of individual thread run times).

<div class="wrap-wider-child">
	<img class="width-100 center-block" src="https://philthompson.me/s/img/2024/20240119-run-time-n18.png"/>
</div>

You can see the "reversed" options are always at least as fast and cheap as "nonrev," and often faster and cheaper, so we'll just ignore the "nonrev" options.

You can also see that the overall running time drops as we add more threads until we reach the fastest possible running time.  For <math style="font-size:1.2rem"><mi>n</mi><mo>=</mo><mn>18</mn></math>, that happens with 25 threads.

But since Amazon only offers 8, 16, and 32-vCPU `c7g` instances that are relevant here, we'll consider just those.  We see that while running the calculation on an 8-vCPU instance is slightly cheaper than the 16-vCPU, it's slower.  The 32-vCPU is a little faster than the 16 (remember than the blue line is plotted on a log scale), but it's considerably more expensive.

As we go from <math style="font-size:1.2rem"><mi>n</mi><mo>=</mo><mn>18</mn></math> to <math style="font-size:1.2rem"><mi>n</mi><mo>=</mo><mn>21</mn></math>, we increase the number of thread tasks, but the 

Again, I made these charts after having selected the 16-vCPU instances for my <math style="font-size:1.2rem"><mi>n</mi><mo>=</mo><mn>21</mn></math> and <math style="font-size:1.2rem"><mi>n</mi><mo>=</mo><mn>22</mn></math> calculations, and I'm relieved it showed I made a sensible choice.  While Amazon does offer some 24-vCPU instances, they are much more expensive on a per-vCPU basis than the hypothetical 24-thread `c7g` instance plotted here.

### Charting for <math style="font-size:1.4rem"><mi>n</mi><mo>=</mo><mn>21</mn></math>

*I am assembling the final charts for running times for <math style="font-size:1.2rem"><mi>n</mi><mo>=</mo><mn>21</mn></math> and will post them here.*

<!--
Now let's look at the same chart, but for <math style="font-size:1.2rem"><mi>n</mi><mo>=</mo><mn>21</mn></math>:

<div class="wrap-wider-child">
	<img class="width-100 center-block" src="https://philthompson.me/s/img/2024/20240119-run-time-n21.png"/>
</div>
## AWS Running Times
-->

## Conclusions

This was a terrific winter programming project.  I enjoyed working on this challenging problem, and optimizing my code (even if it was subsequently blown out of the water by other solutions!) similar to my winter of 2022 <a target="_blank" href="${SITE_ROOT_REL}/2022/Advent-of-Code-2022.html">slog through the Advent of Code</a>.

I got to dust off my skills with rust and AWS.  On the rust side, working with threads was quite easy (compared to my similar efforts in python), and I learned a little about pointer arithmetic.  It's too much fun to browse AWS EC2 instance types and just launch them at a whim... there's a lot of computing power out there just waiting to be tapped into.

Lastly, and I enjoyed watching the individual thread tasks complete, one by one as the days crept by, for the final <math style="font-size:1.2rem"><mi>n</mi><mo>=</mo><mn>21</mn></math> count to be revealed.

## Credits

All credit for enumerating polycubes of <math style="font-size:1.2rem"><mi>n</mi><mo>=</mo><mn>21</mn></math> goes to <a target="_blank" href="https://oeis.org/wiki/User:Stanley_Dodds">Stanley Dodds</a> and their breakthrough algorithm.

Dr. Mike Pound and the <a target="_blank" href="https://www.youtube.com/watch?v=g9n0a0644B4">Computerphile channel on YouTube</a>, and all the contributors at the associated <a target="_blank" href="https://github.com/mikepound/opencubes/">GitHub repo</a>, were of course the main inspiration for my work on this project.

