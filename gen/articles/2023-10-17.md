
<!-- Copyright 2023 Phil Thompson. All Rights Reserved.  As noted in the License section of this repository's readme.md file, this file and its corresponding public HTML file, and all other articles, article files, and images, are distributed under traditional copyright.  The repository source code and other files are distributed under the MIT license. -->

[//]: # (gen-title: NFL Elo Models Compared)

[//]: # (gen-title-url: NFL-Elo-Models-Compared)

[//]: # (gen-keywords: NFL, football, power ranking, elo rating, 2023 season)

[//]: # (gen-description: Comparing the accuracy of different Elo rating models for NFL teams.)

[//]: # (gen-meta-end)

Yesterday, I published an update to my NFL Elo rating model, and today I published the "after week 6" ratings and rankings.

In this post, I'd like to compare the accuracy stats for three different models.

[more](more://)

<style>
	table {
		border-collapse: collapse;
		margin-left: auto;
		margin-right: auto;
	}
	td {
		border-top: 1px solid #999;
		text-align: center;
		padding: 0.2rem;
	}
	th {
		text-align: center;
		padding: 0.2rem 1.0rem;
	}
</style>

_Edit 2023-11-12: this post has been updated with the "margin of victory differential" table converted back to football points, which reveals that the "Bigger K" model is in fact better both in terms of picking winners and for estimating margins of victory._

### The Models

* **Bigger K**
  * This is the newest model, featuring a bigger "K-factor" (<a target="_blank" href="https://en.wikipedia.org/wiki/Elo_rating_system#Mathematical_details">see Wikipedia</a>) to be more responsive to individual games.
* **Original**
  * This is the latest version of the initial model, featuring a smaller "K-factor"
* **Blank Slate**
  * This model completely resets all teams to a 1500 rating ahead of week 1

### The Error Rates

#### Number of Game Winners Picked

For games since week 1 of the 2013 season, below we see the number of game winners correctly picked by each model.  To compare early-season vs late-season, this data is broken down by week.  The first row, for example, shows the number of game winners correctly picked on all weeks (on and after week 1).  The 8th row only counts games on and after week 8, about mid-way through the season.

<table>
<tr><th colspan="5">Number of Game Winners Picked by Model</th></tr>
<tr><th>Games Count</th><th>Game Weeks</th><th>Bigger K</th><th>Original</th><th>Blank Slate</th></tr>
<tr><td>2800</td><td>on+after wk. 1</td><td>1832</td><td>1832</td><td>1724</td></tr>
<tr><td>2625</td><td>on+after wk. 2</td><td>1722</td><td>1718</td><td>1638</td></tr>
<tr><td>2449</td><td>on+after wk. 3</td><td>1609</td><td>1606</td><td>1534</td></tr>
<tr><td>2273</td><td>on+after wk. 4</td><td>1500</td><td>1504</td><td>1438</td></tr>
<tr><td>2106</td><td>on+after wk. 5</td><td>1397</td><td>1396</td><td>1346</td></tr>
<tr><td>1945</td><td>on+after wk. 6</td><td>1290</td><td>1288</td><td>1250</td></tr>
<tr><td>1786</td><td>on+after wk. 7</td><td>1188</td><td>1186</td><td>1158</td></tr>
<tr><td>1643</td><td>on+after wk. 8</td><td>1091</td><td>1090</td><td>1064</td></tr>
<tr><td>1502</td><td>on+after wk. 9</td><td>988</td><td>983</td><td>974</td></tr>
<tr><td>1369</td><td>on+after wk. 10</td><td>909</td><td>904</td><td>896</td></tr>
<tr><td>1231</td><td>on+after wk. 11</td><td>830</td><td>827</td><td>817</td></tr>
<tr><td>1090</td><td>on+after wk. 12</td><td>728</td><td>728</td><td>719</td></tr>
<tr><td>937</td><td>on+after wk. 13</td><td>622</td><td>626</td><td>609</td></tr>
<tr><td>782</td><td>on+after wk. 14</td><td>520</td><td>524</td><td>513</td></tr>
<tr><td>627</td><td>on+after wk. 15</td><td>420</td><td>421</td><td>415</td></tr>
<tr><td>467</td><td>on+after wk. 16</td><td>311</td><td>313</td><td>308</td></tr>
</table>

We can see that the "Bigger K" model picks the same number of game winners as the "Original" model, and generally does a better job toward the beginning and middle of the season while the "Original" model does a better job picking winners in the last few weeks of the season.

The "Blank Slate" model picks far fewer game winners correctly in the beginning of the season, but by the end of the season, as expected, performs similarly to the other models (though still a bit worse).

#### Margin of Victory Differential

The "Margin of Victory Differential" table below shows how accurate each model's expected margin of victory is.  It shows the median value over all tested games, again since week 1 of the 2013 season.

The "differential" is between the expected margin of victory and the actual margin of victory (in football points) for each football game.  The model uses each team's rating, home-vs-away, etc., to calculate an expected "Elo score" for both teams.  The "Elo score" is on a 0.0 to 1.0 scale.  A team that gets "blown out" earns a 0.0, teams that tie earn a 0.5, and a team that wins in a blowout earns a score of 1.0.  The sum of both teams' scores is 1.0.  Using each model's parameters for "close victory" and "blowout," these 0.0-1.0 expected scores are converted to football points.

For example, if "team A" is expected to win by 7 points, and ends up winning the game by 10 points, the "margin of victory differential" is 3 points.  If "team A" ends up winning by 3 points, the differential is 4 points.  The differential also includes the expected winner vs actual winner: if "team A" is expected to win by 3 points, and ends up actually losing by 3 points, the "margin of victory differential" is 6 points.

A median "Margin of victory differential" of 8.0 means that just as many games had an actual margin of victory fewer than 8 points away from the estimated margin as had more.  In other words, the estimated margins of victory/defeat were fewer than 8 points away from the actual margins of victory/defeat just as often as they were more than 8 away from the actual margin.  (If I think of a better way to word this I'll edit this post.)

<table>
<tr><th colspan="5">Median Margin of Victory Differential by Model</th></tr>
<tr><th>Games Count</th><th>Game Weeks</th><th>"Bigger K"</th><th>"Original"</th><th>"Blank Slate"</th></tr>
<tr><td>2800</td><td>on+after wk. 1</td><td>8.0</td><td>8.0261</td><td>8.0636</td></tr>
<tr><td>2625</td><td>on+after wk. 2</td><td>8.0417</td><td>8.0915</td><td>8.1326</td></tr>
<tr><td>2449</td><td>on+after wk. 3</td><td>8.1496</td><td>8.1194</td><td>8.2433</td></tr>
<tr><td>2273</td><td>on+after wk. 4</td><td>8.078</td><td>8.0382</td><td>8.1103</td></tr>
<tr><td>2106</td><td>on+after wk. 5</td><td>8.0</td><td>8.0029</td><td>8.049</td></tr>
<tr><td>1945</td><td>on+after wk. 6</td><td>8.0</td><td>8.0114</td><td>8.1113</td></tr>
<tr><td>1786</td><td>on+after wk. 7</td><td>8.0</td><td>8.016</td><td>8.1296</td></tr>
<tr><td>1643</td><td>on+after wk. 8</td><td>8.0</td><td>8.0</td><td>8.0604</td></tr>
<tr><td>1502</td><td>on+after wk. 9</td><td>8.0</td><td>8.0081</td><td>8.0829</td></tr>
<tr><td>1369</td><td>on+after wk. 10</td><td>8.0</td><td>8.0</td><td>8.1103</td></tr>
<tr><td>1231</td><td>on+after wk. 11</td><td>7.9073</td><td>8.0</td><td>8.0604</td></tr>
<tr><td>1090</td><td>on+after wk. 12</td><td>8.0106</td><td>8.0757</td><td>8.3364</td></tr>
<tr><td>937</td><td>on+after wk. 13</td><td>8.2963</td><td>8.1807</td><td>8.3774</td></tr>
<tr><td>782</td><td>on+after wk. 14</td><td>8.3071</td><td>8.2555</td><td>8.3521</td></tr>
<tr><td>627</td><td>on+after wk. 15</td><td>8.1665</td><td>8.1368</td><td>8.3381</td></tr>
<tr><td>467</td><td>on+after wk. 16</td><td>8.4544</td><td>8.4859</td><td>8.654</td></tr>
</table>

The new "Bigger K" model is a little better at predicting margins of victory/defeat than the "Original" model.  The "Blank Slate" model surprisingly gets worse toward the end of the season, and overall it performs the worst. The "Bigger K" model is the best at most points in the season.

I'm not sure how meaningful this margin of victory differential is.  It seems to me that predicting blowouts properly, and close games, is a sign of an accurate model so that's why I've included it in the post and why I use it when trying to tune a model for accuracy.

### Conclusions

The "Blank Slate" model is fun to look at, but is clearly less accurate.  It picks 108 fewer game winners than the other models since 2013.

The "Original" model is more conservative.  It effectively gives historically "good" teams the benefit of the doubt, and doesn't drop their Elo rating too much after a few bad games.  As a result, it tends to be a hair more accurate in the later stages of the season than the "Bigger K" model.

The "Bigger K" model however reacts more strongly to individual games, and passes the "eye test" much more convincingly than the "Original" model, especially in the early stages of a season.  Accordingly, it does pick more winners in the beginning and middle of the season than the "Original" model.  It also is the best model at estimating margin of victory.

Again, <a href="${SITE_ROOT_REL}/nfl-elo/2023.html">here is the power rankings page for the 2023 season</a>.  Oh, and <a href="${SITE_ROOT_REL}/nfl-elo/2023-only.html">here are the 2023-only rankings</a> that use the "Blank Slate" model.
