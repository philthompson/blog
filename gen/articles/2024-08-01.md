
<!-- Copyright 2024 Phil Thompson. All Rights Reserved.  As noted in the License section of this repository's readme.md file, this file and its corresponding public HTML file, and all other articles, article files, and images, are distributed under traditional copyright.  The repository source code and other files are distributed under the MIT license. -->

[//]: # (gen-title: NFL Elo Power Rankings for 2024)

[//]: # (gen-title-url: NFL-Elo-Power-Rankings-for-2024)

[//]: # (gen-keywords: NFL, football, power ranking, elo rating, 2024 season)

[//]: # (gen-description: Retrospective on the Elo ratings for the 2023 season, and changes for the 2024 season.)

[//]: # (gen-meta-end)

<style>
  table {
    border-collapse: collapse;
    margin-left: auto;
    margin-right: auto;
  }
  td {
    border-top: 1px solid #999;
    text-align: center;
    padding: 0.1rem 1.25rem 0.1rem 1.25rem;
  }
  /*.season-stat-li {
    padding-bottom: 1.5rem;
  }
  .row-toggle-and-table td a, .season-stat-li a {
    color: inherit;
    text-decoration: none;
    transition: all .3s ease;
    height: inherit;
    line-height: inherit;
  }
  .row-toggle-and-table td a:hover, .season-stat-li a:hover {
    color: white;
  }*/
  th {
    text-align: center;
    font-size: 0.7rem;
    padding: 0.1rem 1.25rem 0.1rem 1.25rem;
  }
  /*.row-toggle-and-table {
    margin: 10px 0;
    position: relative;
    text-align: center;
  }
  input[type="checkbox"] {
    width: 100%;
    height: 2.0rem;
    position: absolute;
    left: 0;
    top: 0;
    opacity: 0;
    z-index: 10;
    cursor: pointer;
  }
  input[type="checkbox"]:hover ~ a {
    color: white;
    text-decoration: none;
  }*/
  .row-toggle-and-table a {
    position: relative;
    height: 2.0rem;
    line-height: 2.0rem;
    display: inline-block;
    /* background:red; */
    /* color:white; */
    transition: all .3s ease;
    padding: 0.0rem 1.0rem;
    /* width:100%; */
    text-decoration: underline;
  }
  /*.row-toggle-and-table a+a, .collapsible {
    display:none;
    transition:all .3s ease;
  }
  .row-toggle-and-table input[type="checkbox"]:checked ~ a {
    display:none;
  }
  .row-toggle-and-table input[type="checkbox"]:checked ~ a+a {
    display:inline-block;
  }
  .row-toggle-and-table input[type="checkbox"]:checked ~ table tr.collapsible {
    display:table-row;
  }
  tr.collapsible {
    line-height: 0.7rem;
  }
  .collapsible td {
    text-align: left;
    border: 0px;
    padding: 0rem 0.75rem 0.4rem 0.75rem;
  }*/
  td.left {
    text-align: left;
  }
  /*span.slw {
    opacity: 35%;
    font-size: 0.75rem;
  }
  .top-arw {
    opacity: 0.5;
    text-decoration: none !important;
  }*/
</style>

<a href="${THIS_ARTICLE}"><img style="float: left" class="width-resp-50-100" src="${SITE_ROOT_REL}/s/img/2024/nfl-elo-2024.jpg"/></a> The 2024 NFL season is here!

I've been working on Elo model improvements, off and on, for the entire offseason.  I'm excited to share
some improvements.

The new <a href="${SITE_ROOT_REL}/nfl-elo/2024.html">2024 season ratings and rankings page</a> is live!

(The <a href="${SITE_ROOT_REL}/nfl-elo/2023.html">2023</a>, <a href="${SITE_ROOT_REL}/nfl-elo/2022.html">2022</a>,
and <a href="${SITE_ROOT_REL}/nfl-elo/2021.html">2021</a> pages are available and updated with the new model
described below.) 

[more](more://)

<p style="clear:both">&nbsp;</p>

## <a name="elo-model-background"></a>Elo model Background

Before getting started, I'd like to explain why I think an Elo rating model, and one that uses margin of
victory, is useful for ranking NFL teams.

The <a target="_blank" href="https://en.wikipedia.org/wiki/Elo_rating_system">Elo rating system</a> was
created to compare the playing strength of chess players.  Each player earns a numeric rating, where an
average player would earn a rating of 1,500 and very strong players can earn a rating well over 2,500.
Over a series of one or more games, against one or more opponents, a player's *expected* winning percentage
can be calculated.  Following the game(s) the difference between the actual and expected winning percentages
are used to calculate an updated rating.  If a player does better than expected, their rating increases. If
they perform below expectations, their rating decreases.  If they play as expected, their rating remains
the same.

In applying this system to the NFL, we can't treat an NFL game like a single chess game.  From the opening
moves, every move in a chess game is strongly influenced by the preceding moves.  And after a few early
blunders, a chess player can lose the game having played only a few moves.  Neither of these aspects of
chess apply to an NFL game.  After a scoring drive, an NFL team kicks to their opponent and to some degree
the game is reset and a new drive begins &mdash; nothing like this happens in a single chess game.

A better way to apply Elo to the NFL is to consider an NFL game analogous to a series of chess games, perhaps
like a multi-game chess match.  Over a series of chess games there will be some strategy adjustment
but the individual games will be largely independent.  Similarly, an NFL game is comprised of largely
independent *drives*.  In order to increase the likelihood of winning the game, NFL offenses generally attempt
to score on every drive.  There are exceptions to this, such as drives where the score is already lopsided
or where a team attempts to run out the clock rather than score, but for the most part each drive is
somewhat independent: the offense is trying to score, and the defense is trying to prevent a score.

If we use the drives analogy, the cumulative result of the <a target="_blank" href="https://fivethirtyeight.com/features/nfl-games-have-never-been-closer-heres-why/"> 10+ drives per game per team</a> is the margin of victory.  Where
a "won" drive can be a score on offense, preventing a score on defense, pinning the opponent back with a
good punt, or running out the clock successfully, a team that "wins" more drives than its opponent will likely
win and likely end up with a larger margin of victory.

In continuing this analogy, in place of computing an expected winning percentage over a series of chess games,
for an NFL game we can compute an expected margin of victory (or margin of defeat) for each team.  Following
the NFL game both teams' Elo ratings are updated based on their actual-vs-expected margin of victory.  There
are two common situations worth mentioning here.  First, if the winning team wins by a smaller than expected
margin, they will lose Elo rating points.  Secondly, if a team loses but by a smaller than expected margin,
they will gain Elo rating points.  Neither of these can happen following a *single* chess game, but again, we
are not treating an NFL game like a single chess game.  After a series of games in a multi-game match or
tournament, chess players can also, for example, win a match with a lower than expected winning percentage
and lose Elo rating points.

One caveat here is that there are many more factors that must be taken into account to increase the accuracy
of an Elo model when applying it to NFL games.  The model used here uses the home/away/neutral venue status
for each team, the number of rest days, and a few other factors, when calculating the expected margin of
victory for each team.  These factors, as well as parameters defining a "close win," "blowout win," etc.,
are all weighted to produce a model that picks the most game winners.

I mentioned earlier that there are situations where an NFL team will intentionally try *not* to score (or in
rare cases even allow the opponent to easily score!) in an effort to increase their odds of winning the game.
In order to not penalize teams for these strategies we cannot, for example, treat a 70-point victory as being
10 times as valuable as a 7-point victory.  Instead, we have to reward margin of victory on a curve where
proportionally more value is placed on each additional point of margin of victory in narrow wins than on
each additional point in bigger wins.  No additional value is awarded on scoring more points in blowout
wins.  More background on implementing the curve was given
<a href="${SITE_ROOT_REL}/2023/NFL-Elo-Power-Rankings-for-2023.html#update-2023-11-15">in an update on the 2023 NFL Elo post</a>.

## Power Rankings vs Elo Ratings

Once we have Elo ratings for each NFL team, we can simply arrange them in order to get our power ranking.

For any two teams, their Elo ratings are a measure of how they are expected for perform against each other at
a neutral site, with the same number of days' rest since the last game, etc.

To clarify: to compare two teams we must look at their Elo ratings, not their power rankings.  Take for example
one team, ranked #5 in the league with an Elo rating of 1530, and another team at #9 in the league with an
Elo rating of 1528.  At 1530 and 1528, those teams *are very close* in Elo rating.  The fact that they are ranked
5th and 9th does not matter.  Teams with ratings only a few Elo points apart like this are expected to be roughly
the same strength.  A game between these hypothetical teams would likely be a "Tossup" without a
meaningful margin of victory expected for either team.

## 2023 Retrospective

Let's look at how the 2023 season Elo ratings compared to playoff seeding.  Matching the playoff
seeding is not the goal of these Elo ratings, but it's interesting to see how closely
they matched at the end of the regular season.

For the AFC playoff bracket, the 1-3 seeds were all in Elo
order, and aside from the Texans, the other AFC playoff teams were seeded very close to
their Elo ranking among AFC teams.  The Texans had a very good season Elo-wise, and aside
from their week 14 loss to the Jets they steadily increased in Elo rating throughout the
season.  As division winners they earned the #4 AFC playoff seed, far ahead of their 12th-best
AFC Elo rating.

<table>
  <tbody>
    <tr>
      <!--<th class="left" style="padding-left:0rem">Elo Rank</th>-->
      <th class="left" style="padding-left:0">AFC Elo Rank</th>
      <th class="left" style="padding-left:2.0rem">AFC Playoff Seed</th>
      <th>Team</th>
      <th>Record</th>
    </tr>
    <tr>
      <!--<td class="left">1</td>-->
      <td class="left">1</td>
      <td class="left">1</td>
      <td>Baltimore Ravens</td>
      <td>13-4</td>
    </tr>
    <tr>
      <!--<td class="left">4</td>-->
      <td class="left">2</td>
      <td class="left">2</td>
      <td>Buffalo Bills</td>
      <td>11-6</td>
    </tr>
    <tr>
      <!--<td class="left">5</td>-->
      <td class="left">3</td>
      <td class="left">3</td>
      <td>Kansas City Chiefs</td>
      <td>11-6</td>
    </tr>
    <tr>
      <!--<td class="left">23</td>-->
      <td class="left">12</td>
      <td class="left">4</td>
      <td>Houston Texans</td>
      <td>10-7</td>
    </tr>
    <tr>
      <!--<td class="left">14</td>-->
      <td class="left">7</td>
      <td class="left">5</td>
      <td>Cleveland Browns</td>
      <td>11-6</td>
    </tr>
    <tr>
      <!--<td class="left">6</td>-->
      <td class="left">4</td>
      <td class="left">6</td>
      <td>Miami Dolphins</td>
      <td>11-6</td>
    </tr>
    <tr>
      <!--<td class="left">15</td>-->
      <td class="left">8</td>
      <td class="left">7</td>
      <td>Pittsburgh Steelers</td>
      <td>10-7</td>
    </tr>
  </tbody>
</table>

The NFC seeding was also quite close to the NFC Elo rankings.  Tampa Bay won their
division and earned the #4 seed, boosting them well ahead of their 7th NFC Elo ranking.
The Eagles' Elo rating had collapsed by the end of the season, but their early-season
wins meant they earned the #5 seed while only having the 8th-best NFC Elo rating. 

<table>
  <tbody>
    <tr>
      <!--<th class="left" style="padding-left:0rem">Elo Rank</th>-->
      <th class="left" style="padding-left:0">NFC Elo Rank</th>
      <th class="left" style="padding-left:2.0rem">NFC Playoff Seed</th>
      <th>Team</th>
      <th>Record</th>
    </tr>
    <tr>
      <!--<td class="left">2</td>-->
      <td class="left">1</td>
      <td class="left">1</td>
      <td>San Francisco 49ers</td>
      <td>12-5</td>
    </tr>
    <tr>
      <!--<td class="left">3</td>-->
      <td class="left">2</td>
      <td class="left">2</td>
      <td>Dallas Cowboys</td>
      <td>12-5</td>
    </tr>
    <tr>
      <!--<td class="left">7</td>-->
      <td class="left">3</td>
      <td class="left">3</td>
      <td>Detroit Lions</td>
      <td>12-5</td>
    </tr>
    <tr>
      <!--<td class="left">12</td>-->
      <td class="left">7</td>
      <td class="left">4</td>
      <td>Tampa Bay Buccaneers</td>
      <td>9-8</td>
    </tr>
    <tr>
      <!--<td class="left">16</td>-->
      <td class="left">8</td>
      <td class="left">5</td>
      <td>Philadelphia Eagles</td>
      <td>11-6</td>
    </tr>
    <tr>
      <!--<td class="left">6</td>-->
      <td class="left">5</td>
      <td class="left">6</td>
      <td>Los Angeles Rams</td>
      <td>10-7</td>
    </tr>
    <tr>
      <!--<td class="left">15</td>-->
      <td class="left">6</td>
      <td class="left">7</td>
      <td>Green Bay Packers</td>
      <td>9-8</td>
    </tr>
  </tbody>
</table>

Before the Super Bowl, the 1583-rated 49ers were the "Slight favorite" over the 1565-rated
Chiefs at the neutral Super Bowl site.  Given that the game went to overtime I don't think
this Elo rating-based expectation was too far off.

## <a name="post-2023-frozen-pages"></a>Post-2023 "Frozen" Pages

Finally, before moving onto changes for 2024, I'd like to share links to "frozen" versions of the rankings pages as they appeared at the end of last season:

<p><a href="${SITE_ROOT_REL}/nfl-elo/2021-frozen-Feb-2024.html">Frozen as of February 2024: 2021</a></p>
<p><a href="${SITE_ROOT_REL}/nfl-elo/2022-frozen-Feb-2024.html">Frozen as of February 2024: 2022</a></p>
<p><a href="${SITE_ROOT_REL}/nfl-elo/2023-frozen-Feb-2024.html">Frozen as of February 2024: 2023</a></p>

<p><a href="${SITE_ROOT_REL}/nfl-elo/2021-only-frozen-Feb-2024.html">Frozen as of February 2024: 2021-only ("blank slate" model)</a></p>
<p><a href="${SITE_ROOT_REL}/nfl-elo/2022-only-frozen-Feb-2024.html">Frozen as of February 2024: 2022-only ("blank slate" model)</a></p>
<p><a href="${SITE_ROOT_REL}/nfl-elo/2023-only-frozen-Feb-2024.html">Frozen as of February 2024: 2023-only ("blank slate" model)</a></p>

My intention with these "frozen" pages is to keep them as they originally appeared at the time of freezing.  I may
want to do model updates every offseason, and in that case I'd like to keep a record of the old Elo models' rankings
and ratings.

## 2024 Changes

For the 2024 season, the Elo rating system is moving to a substantially more accurate model.  I am calling this new model `v2.2024.07` since it was discovered using a new process detailed below.

The best Elo model used during the 2023 season correctly picks 2,118 out of 3,259 game winners (65.0%) over the 12 seasons from 2012-2023.  The new `v2.2024.07` model picks 2,161 of 3,259 game winners (66.3%) over that span, which averages a net of 3.58 more correctly-picked games per season.

Next, I'll give some details on how this new model was found.

## <a name="embracing-randomness"></a>2024 Model Updates: Embracing Randomness

My <a target="_blank" href="${SITE_ROOT_REL}/2023/NFL-Elo-Power-Rankings-for-2023.html#update-2023-10-16">goal from last year</a> hasn't changed:

> "I want to publish only the best, most accurate, and therefore most useful, Elo ratings. I was not and still am not willing to change the model to create power rankings that look better but are less accurate."

For the 2023 season, while searching for the best (most accurate) Elo model to use, I tested various models with parameter settings that made intuitive sense to me.  I could try a few ideas, see which models worked best, and "fine tune" the good ones by hand (honing in on exact fractional values that ended up picking more game winners correctly).  A few times during the season, following feedback from my posts on reddit, I went back to the drawing board and ended up finding better models.  Details for those updates were published on the <a target="_blank" href="${SITE_ROOT_REL}/2023/NFL-Elo-Power-Rankings-for-2023.html">2023 version of this page</a>.

For the upcoming 2024 season, again with the goal of finding the best possible Elo model, I wanted to test many many more models.

I realized early on that there is no way to systematically test every possible model.  The Elo model used in this project has well over 20 parameters, many of which are decimal numbers for which a tiny adjustment can have a drastic effect on the accuracy of the model.  On top of that the parameters seem to interact in chaotic, unpredictable ways.   

I love algorithms involving randomness, so I implemented one to search for Elo models.  The basic idea is to simply generate randomized models and test them over the 12 years' worth of games.  Most random models will be rubbish, but some are quite good.  Given that I don't know which values for which parameters will result in the best model, and that I don't want to impose any of my biases upon the model, I'm not sure there is a better way to search for the best possible model other than through a randomized search method similar to this.  Aside from some bias sneaking in via the lower and upper bounds on each parameter's search space, I think this search method satisfies my goal of looking for and using the best possible model for this Elo rating system.

After some initial testing, I found I can generate and test thousands of models every minute (per CPU core).  In order to find the "best" models among the millions I'd be generating, I needed to find a way to sort them.

Models are compared on two criteria:

* number of game winners correctly picked:
  * after adjustments for rest days, home vs. away, and so on, whichever team has the higher expected performance from 0.0 (blowout loss) to 1.0 (blowout win) is the modeled winner
  * if, after these adjustments, both competing teams' expected performance rounds to 0.5, then *neither* is the "expected" winner
    * if the game ends up in a tie, then the model is awarded a "correctly picked" game, or
      * for the new model this only occurred for WAS@NYG, 2022 wk. 13
    * if the game goes to overtime, and the team with the higher expected performance wins, the model is awarded a "correctly picked" game
      * even a very slight expected advantage still counts as a correct pick
    * if the game goes to overtime, and the team with the *lower* expected performance wins, the model is awarded 1/2 of a "correctly picked" game
      * for the new model this occurred for TB@CAR, 2012 wk. 11, and IND@DEN, 2022 wk. 5
  * the last week of each regular season is omitted from the model comparison because very strong teams will often intentionally rest players and lose to weaker teams
* standard deviation of correctly-picked winners percentage, across all seasons
  * the percentage is the above "winners correctly picked" count divided by the number of games in the season
    * omitting last week of regular season since it's not helpful for comparing models when strong teams, with inconsequential final games, sometimes rest starters
    * where percentages are scaled from 0.0% to 100.0%
  * I will abbreviate this "stddev." from here on

Since more correctly-picked game winners is better, and a lower stddev. is better, these two criteria are combined into a single value for sorting with the formula:

<math display="block" style="font-size:1.5rem">
  <mrow>
    <mtext>winners</mtext><mo>-</mo><mo>(</mo><mtext>stddev</mtext><mo>*</mo><mn>5.5</mn><mo>)</mo>
  </mrow>
</math>

I came up with this formula by generating a ton of randomized winners/stddev. pairs, and sorting them according to various formulas.  The above formula seemed to place models in an order that I agreed with.  For example, a model that picks `2,030` game winners correctly over several seasons, with a stddev. of `1.9%` across those seasons, is **inferior** to a model that picks one fewer game winner correctly, `2,029`, but has a lower stddev. of `1.7%`.

*Note: By including the stddev. criterion I should be avoiding most problems with overfitting, because models that more consistently pick winners are more highly-ranked.  Either way, I'm not too concerned with overfitting since I expect to repeat this same model search next offseason.  If this year's model is overfitted to the 2010-2023 seasons, at the expense of 2024 accuracy, then I'd expect some other model to be better overall from 2010-2024.*

I was surprised to see how quickly randomized models were found that were better than my best hand-adjusted model!  My best hand-adjusted model used during the 2023 season correctly picks 2,118 out of 3,259 game winners (65.0%) over the 12 seasons from 2012-2023. The best models seemed to asymptotically approach 2,150 winners correctly picked, with many reaching the 2,140 mark and only a few here and there surpassing 2,145 (65.8%).

It was fun to let the model search run for a few hours (or days) at a time, and check in on its best found models.

## 2024 Model Updates: Merging Models

Inspired by genetic algorithms, I next tried the idea of merging pairs of good models to see if any better "child" models could be found &mdash; and it worked!  Pairs of models have around 20 parameters that are different between them, which means there are 2<sup>20</sup> (around 1 million) "child" models, with every possible combination of those parameters, to evaluate for each pairing.  Depending on how aggressively inferior-looking models are dropped, this can take anywhere from 1 to 5 full days to run on my computer for each model pair, but it only needed to be run a handful of times when a really promising new model was discovered.

On June 12, 2024, a combination of parameters from a 2,147 game winners model, merged with another model with slightly fewer winners but better stddev., produced a "child" model that picks 2,150 winners correctly (66.0%).

On July 8, 2024, another pair of models, both with fewer than 2,150 winners picked, produced a "child" model that picks 2,158 winners correctly (66.2%).  This was the first model to break the 66% pick rate!

On July 27, 2024, the above model was merged with a new low-stddev. model (itself merged from new two low-stddev. models).  Two child models were found that were better than all the rest.  One with 2,161 picked winners (66.3%) and one with 2,159 winners but a better stddev. number.  In the end, I decided to go with the 2,161 winners model because it does beat out the 2,159 model with both the overarching "more winners" criterion and the above model sorting formula.

With training camp and preseason games rapidly approaching, and more work needing to be done on season page improvements and backend verification, I declared this the final model found for the 2024 offseason.  I am calling this new model `v2.2024.07` with a `release.year.month` format so I can tell at a glance which model I am using and when it was found.

## 2024 Model Updates: Notable Models Compared

Here is a comparison, by season, of the best 2023 model and the `v2.2024.07` model:

<p class="wrap-wider-child"><a target="_blank" href="${SITE_ROOT_REL}/s/img/2024/20240801-By-Season-Best-2023-Model-vs-New-2024-Model.png"><img class="width-100 center-block" src="${SITE_ROOT_REL}/s/img/2024/20240801-By-Season-Best-2023-Model-vs-New-2024-Model.png"/></a></p>

Except for 2020 and 2021 the new model is a very good improvement when looking
at overall pick percentage by seasons.  Even 2014, which was a very predictable season for the
previous model, was improved upon by this new model.

And here is the comparison by week of season:

<p class="wrap-wider-child"><a target="_blank" href="${SITE_ROOT_REL}/s/img/2024/20240801-By-Week-Best-2023-Model-vs-New-2024-Model.png"><img class="width-100 center-block" src="${SITE_ROOT_REL}/s/img/2024/20240801-By-Week-Best-2023-Model-vs-New-2024-Model.png"/></a></p>

We can see the new `v2.2024.07` model
picks more games correctly for most weeks, and when it lags behind the previous model (weeks 5 and 17) it does
so only by a little.  Don't read too much into the last few weeks of the season on this plot.  Pick percentages
can swing wildly because there are so few games played in playoff weeks compared to regular season weeks.

Interestingly, week 10 appears to be universally unpredictable &mdash; even when looking at the
results of a variety of models.  Some weeks give many models trouble, like weeks 3 and 6, but none seem to be as
troublesome for all models as week 10:

<p class="wrap-wider-child"><a target="_blank" href="${SITE_ROOT_REL}/s/img/2024/20240801-By-Week-Notable-Models.png"><img class="width-100 center-block" src="${SITE_ROOT_REL}/s/img/2024/20240801-By-Week-Notable-Models.png"/></a></p>

I believe this week 10 problem may be caused by some aspect of the system I've developed and is not actually
revealing some subtlety of NFL player contracts/incentives, rules around injuries, pension qualifications,
or similar wrinkle I've overlooked.  If you think this week 10 unpredictability might actually have some
cause(s) beyond my Elo model, please let me know.

## <a name="new-blank-slate-model"></a>2024 New "Blank Slate" Model

For the 2023 season, I published both the <a href="${SITE_ROOT_REL}/nfl-elo/2023.html">regular</a> and <a href="${SITE_ROOT_REL}/nfl-elo/2023-only.html">"blank slate"</a>
versions of the rankings pages.  The "blank slate" page used the same model as the regular page,
but with one change: all teams were reset to a 1500 rating every offseason.  The regular model
does adjust all teams a little bit toward the 1500 average every offseason, but the blank slate
version performs the full reset to 1500.

While less accurate than the regular model, the blank slate pages are fun to look at.  By "less
accurate" I am referring to the ability of the model to accurately pick game winners.  In
comparison with the regular model, the blank slate version of the 2023 model was much less
accurate at the beginning of the season (because for the most part, strong teams stay relatively
strong in the subsequent season) and only slightly less accurate at the end of the season (where
both models had nearly converged on the same ratings and rankings for the league, but the
regular model was still slightly more accurate).

For the 2024 season, I've attempted to find a better blank slate model by doing the same randomized
model search as described above, but this time with the full reset to 1500 every offseason.  My guess
was that, given the annual reset, some other set of model parameters would be more accurate than
simply using the rest of the regular model parameters.  I quickly found that this was true, and I was
able to find a model that picks 2,085 of 3,259 game winners from 2012-2023 (64.0%) with a standard
deviation of 2.30% of picked winners percentage across seasons.

Simply taking the best known regular model, which picks 2,161 winners from 2012-2023, and changing
one parameter to get the reset-to-1500 behavior creates a model that picks only 2,026 of 3,259 game
winners over that span (62.2%) with a standard deviation of 2.59% (up from 2.13%) picked winners 
percentage across seasons.

In short: by looking for a better blank slate model, I was able to improve the blank slate page
model from a 62.2% rate of picking winners to 64.0%.  In the next section, I'll show a more
detailed breakdown comparing the two models.

## <a name="regular-vs-blank-slate"></a>Comparing Regular and "Blank Slate" Models

To compare the best found regular and blank slate models, let's first look at picked
winners accuracy by week.  We'd expect the blank slate model to pick about 50% of
winners in the first week, then improve in the following weeks.  We do see what we expect:

<p class="wrap-wider-child"><a target="_blank" href="${SITE_ROOT_REL}/s/img/2024/20240801-By-Week-Regular-vs-Blank-Slate.png"><img class="width-100 center-block" src="${SITE_ROOT_REL}/s/img/2024/20240801-By-Week-Regular-vs-Blank-Slate.png"/></a></p>

It might be just random chance, but the blank slate model appears to be roughly as accurate
as the regular model toward the end of the regular season, but in the playoffs the
regular model is much more accurate.

When comparing the two models by season, the blank slate model does match the regular model
for the 2016, but in general is less accurate:

<p class="wrap-wider-child"><a target="_blank" href="${SITE_ROOT_REL}/s/img/2024/20240801-By-Season-Regular-vs-Blank-Slate.png"><img class="width-100 center-block" src="${SITE_ROOT_REL}/s/img/2024/20240801-By-Season-Regular-vs-Blank-Slate.png"/></a></p>

The data from the above plots is provided here in table form.

<table>
  <tr><th colspan="3" style="font-weight:normal">Game Winners Picked By Week of Season</th></tr>
  <tr>
    <th>Span</th>
    <th class="left" style="padding-left:0"><code>v2.2024.07</code></th>
    <th class="left" style="padding-left:2.0rem"><code>blank-slate-v1.2024.07</code></th>
  </tr>
  <tr><td><small>191 gm on wk 1</small></td><td>121 (63.35%)</td><td>95 (49.74%)</td></tr>
  <tr><td><small>192 gm on wk 2</small></td><td>130 (67.71%)</td><td>126 (65.62%)</td></tr>
  <tr><td><small>192 gm on wk 3</small></td><td>119 (61.98%)</td><td>113 (58.85%)</td></tr>
  <tr><td><small>182 gm on wk 4</small></td><td>118 (64.84%)</td><td>102 (56.04%)</td></tr>
  <tr><td><small>175 gm on wk 5</small></td><td>114 (65.43%)</td><td>111 (63.43%)</td></tr>
  <tr><td><small>173 gm on wk 6</small></td><td>108 (62.43%)</td><td>102 (59.25%)</td></tr>
  <tr><td><small>169 gm on wk 7</small></td><td>113 (66.86%)</td><td>114 (67.46%)</td></tr>
  <tr><td><small>171 gm on wk 8</small></td><td>125 (73.1%)</td><td>124 (72.51%)</td></tr>
  <tr><td><small>161 gm on wk 9</small></td><td>105 (65.22%)</td><td>105 (65.22%)</td></tr>
  <tr><td><small>166 gm on wk 10</small></td><td>95 (57.23%)</td><td>98 (59.04%)</td></tr>
  <tr><td><small>169 gm on wk 11</small></td><td>124 (73.08%)</td><td>124 (73.37%)</td></tr>
  <tr><td><small>185 gm on wk 12</small></td><td>131 (70.81%)</td><td>123 (66.49%)</td></tr>
  <tr><td><small>184 gm on wk 13</small></td><td>118 (64.13%)</td><td>123 (66.85%)</td></tr>
  <tr><td><small>186 gm on wk 14</small></td><td>123 (66.13%)</td><td>122 (65.59%)</td></tr>
  <tr><td><small>192 gm on wk 15</small></td><td>132 (68.75%)</td><td>129 (67.19%)</td></tr>
  <tr><td><small>192 gm on wk 16</small></td><td>129 (67.19%)</td><td>122 (63.54%)</td></tr>
  <tr><td><small>191 gm on wk 17</small></td><td>135 (70.68%)</td><td>137 (71.73%)</td></tr>
</table>

<table style="margin-top:2.0rem">
  <tr><th colspan="3" style="font-weight:normal">Game Winners Picked On+After Week of Season</th></tr>
  <tr>
    <th>Span</th>
    <th class="left" style="padding-left:0"><code>v2.2024.07</code></th>
    <th class="left" style="padding-left:2.0rem"><code>blank-slate-v1.2024.07</code></th>
  </tr>
  <tr><td><small>3,259 gm on+after wk 1</small></td><td>2,161 (66.31%)</td><td>2,085 (63.98%)</td></tr>
  <tr><td><small>3,068 gm on+after wk 2</small></td><td>2,040 (66.49%)</td><td>1,990 (64.86%)</td></tr>
  <tr><td><small>2,876 gm on+after wk 3</small></td><td>1,910 (66.41%)</td><td>1,864 (64.81%)</td></tr>
  <tr><td><small>2,684 gm on+after wk 4</small></td><td>1,791 (66.73%)</td><td>1,751 (65.24%)</td></tr>
  <tr><td><small>2,502 gm on+after wk 5</small></td><td>1,673 (66.87%)</td><td>1,649 (65.91%)</td></tr>
  <tr><td><small>2,327 gm on+after wk 6</small></td><td>1,558 (66.97%)</td><td>1,538 (66.09%)</td></tr>
  <tr><td><small>2,154 gm on+after wk 7</small></td><td>1,450 (67.34%)</td><td>1,436 (66.64%)</td></tr>
  <tr><td><small>1,985 gm on+after wk 8</small></td><td>1,338 (67.38%)</td><td>1,322 (66.57%)</td></tr>
  <tr><td><small>1,814 gm on+after wk 9</small></td><td>1,212 (66.84%)</td><td>1,198 (66.01%)</td></tr>
  <tr><td><small>1,653 gm on+after wk 10</small></td><td>1,108 (67.0%)</td><td>1,092 (66.09%)</td></tr>
  <tr><td><small>1,487 gm on+after wk 11</small></td><td>1,012 (68.09%)</td><td>994 (66.88%)</td></tr>
  <tr><td><small>1,318 gm on+after wk 12</small></td><td>889 (67.45%)</td><td>870 (66.05%)</td></tr>
  <tr><td><small>1,133 gm on+after wk 13</small></td><td>758 (66.9%)</td><td>748 (65.98%)</td></tr>
  <tr><td><small>949 gm on+after wk 14</small></td><td>640 (67.44%)</td><td>624 (65.81%)</td></tr>
  <tr><td><small>763 gm on+after wk 15</small></td><td>517 (67.76%)</td><td>502 (65.86%)</td></tr>
  <tr><td><small>571 gm on+after wk 16</small></td><td>385 (67.43%)</td><td>374 (65.41%)</td></tr>
  <tr><td><small>379 gm on+after wk 17</small></td><td>256 (67.55%)</td><td>252 (66.36%)</td></tr>
</table>

<table style="margin-top:2.0rem">
  <tr><th colspan="3" style="font-weight:normal">Game Winners Picked By Season</th></tr>
  <tr>
    <th>Span</th>
    <th class="left" style="padding-left:0"><code>v2.2024.07</code></th>
    <th class="left" style="padding-left:2.0rem"><code>blank-slate-v1.2024.07</code></th>
  </tr>
  <tr><td><small>267 gm of 2012 season</small></td><td>173.5 (64.98%)</td><td>172.0 (64.42%)</td></tr>
  <tr><td><small>267 gm of 2013 season</small></td><td>181.0 (67.79%)</td><td>179.0 (67.04%)</td></tr>
  <tr><td><small>267 gm of 2014 season</small></td><td>190.0 (71.16%)</td><td>178.5 (66.85%)</td></tr>
  <tr><td><small>267 gm of 2015 season</small></td><td>175.0 (65.54%)</td><td>172.0 (64.42%)</td></tr>
  <tr><td><small>267 gm of 2016 season</small></td><td>176.0 (65.92%)</td><td>176.0 (65.92%)</td></tr>
  <tr><td><small>267 gm of 2017 season</small></td><td>182.0 (68.16%)</td><td>169.0 (63.30%)</td></tr>
  <tr><td><small>267 gm of 2018 season</small></td><td>180.0 (67.42%)</td><td>171.0 (64.04%)</td></tr>
  <tr><td><small>267 gm of 2019 season</small></td><td>174.0 (65.17%)</td><td>169.5 (63.48%)</td></tr>
  <tr><td><small>269 gm of 2020 season</small></td><td>177.0 (65.80%)</td><td>176.0 (65.43%)</td></tr>
  <tr><td><small>285 gm of 2021 season</small></td><td>179.0 (62.81%)</td><td>169.5 (59.47%)</td></tr>
  <tr><td><small>284 gm of 2022 season</small></td><td>189.5 (66.73%)</td><td>180.5 (63.56%)</td></tr>
  <tr><td><small>285 gm of 2023 season</small></td><td>184.0 (64.56%)</td><td>172.0 (60.35%)</td></tr>
</table>

## 2024 Season Page Improvements

Aside from the model updates, during the offseason the following improvements were made to the pages:

* The "Season Stats and Tidbits" section now lists all 32 teams for several stats, where previously only the top 5 teams in each category were shown.
* Added a new "Season Model Accuracy" section showing how many game winners were correctly picked by the model.
* Added buttons to navigate to the previous and next season pages

## 2024 In-season Changes

If the model or pages are changed during the season, I'll post those updates here.